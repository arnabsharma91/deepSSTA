{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "artificial-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb, catboost as cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "valid-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wanted-attachment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polish-worthy",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dried-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_merge_data(data_type=\"training\"):\n",
    "    #\n",
    "    Dfs = []\n",
    "    train_df = []\n",
    "    train_output = pd.read_csv(\"./data_phase_one/training_output.csv\", header=None)\n",
    "    coords = pd.read_csv(\"data_phase_one/coords.csv\", header=None)\n",
    "    for name in [\"mslp\", \"sst\", \"ssta\", \"t2m\"]:\n",
    "        df = pd.read_csv(f\"./data_phase_one/{data_type}_input_{name}.csv\", header=None)\n",
    "        print(\"Shape df\", df.shape)\n",
    "        Dfs.append(df)\n",
    "    j = 0\n",
    "    for i in range(0, df.shape[0]-12, 1):\n",
    "        feat = pd.concat([coords, Dfs[0].iloc[i:i+12], Dfs[1].iloc[i:i+12], Dfs[2].iloc[i:i+12], Dfs[3].iloc[i:i+12]], axis=0).reset_index(drop=True)\n",
    "        tgt = train_output.iloc[i+11]\n",
    "        tgt = tgt.T.reset_index(drop=True)\n",
    "        feat = feat.T.reset_index(drop=True)\n",
    "        feat = pd.concat([feat, tgt], axis=1)\n",
    "        assert feat.shape[1]==51\n",
    "        feat.columns = [f\"feat_{k}\" for k in range(feat.shape[1]-1)] + [\"target\"]\n",
    "        if j < 3:\n",
    "            print(feat.columns)\n",
    "        train_df.append(feat)\n",
    "        j += 1\n",
    "    train_df = pd.concat(train_df, axis=0)\n",
    "    #Dfs.index = [f\"col_{i}\" for i in range(848)]\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "brilliant-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_csv(\"data_phase_one/coords.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "painted-glory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape df (849, 5774)\n",
      "Shape df (849, 5774)\n",
      "Shape df (849, 5774)\n",
      "Shape df (849, 5774)\n",
      "Index(['feat_0', 'feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6',\n",
      "       'feat_7', 'feat_8', 'feat_9', 'feat_10', 'feat_11', 'feat_12',\n",
      "       'feat_13', 'feat_14', 'feat_15', 'feat_16', 'feat_17', 'feat_18',\n",
      "       'feat_19', 'feat_20', 'feat_21', 'feat_22', 'feat_23', 'feat_24',\n",
      "       'feat_25', 'feat_26', 'feat_27', 'feat_28', 'feat_29', 'feat_30',\n",
      "       'feat_31', 'feat_32', 'feat_33', 'feat_34', 'feat_35', 'feat_36',\n",
      "       'feat_37', 'feat_38', 'feat_39', 'feat_40', 'feat_41', 'feat_42',\n",
      "       'feat_43', 'feat_44', 'feat_45', 'feat_46', 'feat_47', 'feat_48',\n",
      "       'feat_49', 'target'],\n",
      "      dtype='object')\n",
      "Index(['feat_0', 'feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6',\n",
      "       'feat_7', 'feat_8', 'feat_9', 'feat_10', 'feat_11', 'feat_12',\n",
      "       'feat_13', 'feat_14', 'feat_15', 'feat_16', 'feat_17', 'feat_18',\n",
      "       'feat_19', 'feat_20', 'feat_21', 'feat_22', 'feat_23', 'feat_24',\n",
      "       'feat_25', 'feat_26', 'feat_27', 'feat_28', 'feat_29', 'feat_30',\n",
      "       'feat_31', 'feat_32', 'feat_33', 'feat_34', 'feat_35', 'feat_36',\n",
      "       'feat_37', 'feat_38', 'feat_39', 'feat_40', 'feat_41', 'feat_42',\n",
      "       'feat_43', 'feat_44', 'feat_45', 'feat_46', 'feat_47', 'feat_48',\n",
      "       'feat_49', 'target'],\n",
      "      dtype='object')\n",
      "Index(['feat_0', 'feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6',\n",
      "       'feat_7', 'feat_8', 'feat_9', 'feat_10', 'feat_11', 'feat_12',\n",
      "       'feat_13', 'feat_14', 'feat_15', 'feat_16', 'feat_17', 'feat_18',\n",
      "       'feat_19', 'feat_20', 'feat_21', 'feat_22', 'feat_23', 'feat_24',\n",
      "       'feat_25', 'feat_26', 'feat_27', 'feat_28', 'feat_29', 'feat_30',\n",
      "       'feat_31', 'feat_32', 'feat_33', 'feat_34', 'feat_35', 'feat_36',\n",
      "       'feat_37', 'feat_38', 'feat_39', 'feat_40', 'feat_41', 'feat_42',\n",
      "       'feat_43', 'feat_44', 'feat_45', 'feat_46', 'feat_47', 'feat_48',\n",
      "       'feat_49', 'target'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = read_merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "brutal-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_more_features(target):\n",
    "    monthly_avg = []\n",
    "    monthly_std = []\n",
    "    for i in range(12):\n",
    "        current_month = []\n",
    "        j = 0\n",
    "        while 5774*12*(j+i)+5774 <= len(target):\n",
    "            current_month.append(target.iloc[5774*12*(j+i):5774*12*(j+i)+5774].values)\n",
    "            #print(current_month[-1].shape)\n",
    "            j += 1\n",
    "        monthly_avg.append(np.stack(current_month, axis=0).mean(0))\n",
    "        monthly_std.append(np.stack(current_month, axis=0).std(0))\n",
    "    return np.concatenate([np.stack(monthly_avg, axis=1), np.stack(monthly_std, axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "excited-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#monthly_stats = get_more_features(data[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "prospective-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"monthly_stats.npy\", monthly_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "environmental-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_stats = np.load(\"monthly_stats.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "geographic-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_stats = np.tile(monthly_stats,(len(data)//5774,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "automotive-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_stats_df = pd.DataFrame(monthly_stats, columns=[f\"add_feat_{i}\" for i in range(24)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dominican-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data.reset_index(drop=True), monthly_stats_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "temporal-compiler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>add_feat_14</th>\n",
       "      <th>add_feat_15</th>\n",
       "      <th>add_feat_16</th>\n",
       "      <th>add_feat_17</th>\n",
       "      <th>add_feat_18</th>\n",
       "      <th>add_feat_19</th>\n",
       "      <th>add_feat_20</th>\n",
       "      <th>add_feat_21</th>\n",
       "      <th>add_feat_22</th>\n",
       "      <th>add_feat_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-64.0</td>\n",
       "      <td>-180.0000</td>\n",
       "      <td>98848.380</td>\n",
       "      <td>99195.690</td>\n",
       "      <td>99320.914</td>\n",
       "      <td>98469.160</td>\n",
       "      <td>98149.220</td>\n",
       "      <td>98269.195</td>\n",
       "      <td>98373.570</td>\n",
       "      <td>97659.090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.590394</td>\n",
       "      <td>0.578681</td>\n",
       "      <td>0.576274</td>\n",
       "      <td>0.548224</td>\n",
       "      <td>0.544279</td>\n",
       "      <td>0.540694</td>\n",
       "      <td>0.538946</td>\n",
       "      <td>0.543125</td>\n",
       "      <td>0.545212</td>\n",
       "      <td>0.543470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-64.0</td>\n",
       "      <td>-177.1875</td>\n",
       "      <td>98898.160</td>\n",
       "      <td>99224.910</td>\n",
       "      <td>99322.336</td>\n",
       "      <td>98480.360</td>\n",
       "      <td>98138.870</td>\n",
       "      <td>98256.570</td>\n",
       "      <td>98282.520</td>\n",
       "      <td>97621.800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600649</td>\n",
       "      <td>0.590663</td>\n",
       "      <td>0.582925</td>\n",
       "      <td>0.571722</td>\n",
       "      <td>0.569716</td>\n",
       "      <td>0.560472</td>\n",
       "      <td>0.563940</td>\n",
       "      <td>0.566644</td>\n",
       "      <td>0.561656</td>\n",
       "      <td>0.562393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-64.0</td>\n",
       "      <td>-174.3750</td>\n",
       "      <td>98945.240</td>\n",
       "      <td>99245.610</td>\n",
       "      <td>99325.310</td>\n",
       "      <td>98495.400</td>\n",
       "      <td>98137.020</td>\n",
       "      <td>98236.720</td>\n",
       "      <td>98203.670</td>\n",
       "      <td>97581.520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.635043</td>\n",
       "      <td>0.606403</td>\n",
       "      <td>0.587164</td>\n",
       "      <td>0.555811</td>\n",
       "      <td>0.557459</td>\n",
       "      <td>0.557803</td>\n",
       "      <td>0.561152</td>\n",
       "      <td>0.562297</td>\n",
       "      <td>0.552638</td>\n",
       "      <td>0.549103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-64.0</td>\n",
       "      <td>-171.5625</td>\n",
       "      <td>98972.610</td>\n",
       "      <td>99259.370</td>\n",
       "      <td>99331.414</td>\n",
       "      <td>98527.730</td>\n",
       "      <td>98152.336</td>\n",
       "      <td>98207.220</td>\n",
       "      <td>98140.990</td>\n",
       "      <td>97533.445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600441</td>\n",
       "      <td>0.585508</td>\n",
       "      <td>0.584608</td>\n",
       "      <td>0.552524</td>\n",
       "      <td>0.554519</td>\n",
       "      <td>0.554806</td>\n",
       "      <td>0.555235</td>\n",
       "      <td>0.559270</td>\n",
       "      <td>0.558470</td>\n",
       "      <td>0.546810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-64.0</td>\n",
       "      <td>-168.7500</td>\n",
       "      <td>98969.490</td>\n",
       "      <td>99280.640</td>\n",
       "      <td>99335.805</td>\n",
       "      <td>98585.020</td>\n",
       "      <td>98189.350</td>\n",
       "      <td>98166.090</td>\n",
       "      <td>98090.790</td>\n",
       "      <td>97491.330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594315</td>\n",
       "      <td>0.586059</td>\n",
       "      <td>0.590216</td>\n",
       "      <td>0.585997</td>\n",
       "      <td>0.589768</td>\n",
       "      <td>0.594207</td>\n",
       "      <td>0.576754</td>\n",
       "      <td>0.578006</td>\n",
       "      <td>0.574201</td>\n",
       "      <td>0.550796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4832833</th>\n",
       "      <td>62.0</td>\n",
       "      <td>-5.6250</td>\n",
       "      <td>100949.125</td>\n",
       "      <td>101105.414</td>\n",
       "      <td>99110.170</td>\n",
       "      <td>100612.875</td>\n",
       "      <td>101548.020</td>\n",
       "      <td>100916.230</td>\n",
       "      <td>101062.870</td>\n",
       "      <td>101320.266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346133</td>\n",
       "      <td>0.347553</td>\n",
       "      <td>0.343245</td>\n",
       "      <td>0.345732</td>\n",
       "      <td>0.347781</td>\n",
       "      <td>0.350528</td>\n",
       "      <td>0.353242</td>\n",
       "      <td>0.354397</td>\n",
       "      <td>0.357285</td>\n",
       "      <td>0.359468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4832834</th>\n",
       "      <td>62.0</td>\n",
       "      <td>-2.8125</td>\n",
       "      <td>101014.790</td>\n",
       "      <td>101204.120</td>\n",
       "      <td>99232.990</td>\n",
       "      <td>100601.810</td>\n",
       "      <td>101610.990</td>\n",
       "      <td>100828.586</td>\n",
       "      <td>101021.590</td>\n",
       "      <td>101323.670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411259</td>\n",
       "      <td>0.412691</td>\n",
       "      <td>0.409359</td>\n",
       "      <td>0.412484</td>\n",
       "      <td>0.412936</td>\n",
       "      <td>0.411689</td>\n",
       "      <td>0.414810</td>\n",
       "      <td>0.418158</td>\n",
       "      <td>0.421191</td>\n",
       "      <td>0.412557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4832835</th>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>101065.984</td>\n",
       "      <td>101295.586</td>\n",
       "      <td>99371.120</td>\n",
       "      <td>100607.914</td>\n",
       "      <td>101676.370</td>\n",
       "      <td>100745.620</td>\n",
       "      <td>100973.234</td>\n",
       "      <td>101342.250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.436968</td>\n",
       "      <td>0.423618</td>\n",
       "      <td>0.420498</td>\n",
       "      <td>0.409317</td>\n",
       "      <td>0.411599</td>\n",
       "      <td>0.408346</td>\n",
       "      <td>0.408299</td>\n",
       "      <td>0.408404</td>\n",
       "      <td>0.408727</td>\n",
       "      <td>0.403374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4832836</th>\n",
       "      <td>62.0</td>\n",
       "      <td>2.8125</td>\n",
       "      <td>101128.520</td>\n",
       "      <td>101377.560</td>\n",
       "      <td>99537.330</td>\n",
       "      <td>100660.810</td>\n",
       "      <td>101771.670</td>\n",
       "      <td>100720.375</td>\n",
       "      <td>100953.950</td>\n",
       "      <td>101417.695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595102</td>\n",
       "      <td>0.583280</td>\n",
       "      <td>0.582017</td>\n",
       "      <td>0.577438</td>\n",
       "      <td>0.579690</td>\n",
       "      <td>0.573577</td>\n",
       "      <td>0.575215</td>\n",
       "      <td>0.565446</td>\n",
       "      <td>0.560426</td>\n",
       "      <td>0.558835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4832837</th>\n",
       "      <td>62.0</td>\n",
       "      <td>19.6875</td>\n",
       "      <td>101012.664</td>\n",
       "      <td>101396.560</td>\n",
       "      <td>100673.860</td>\n",
       "      <td>101356.150</td>\n",
       "      <td>102277.960</td>\n",
       "      <td>101128.240</td>\n",
       "      <td>100753.700</td>\n",
       "      <td>101534.125</td>\n",
       "      <td>...</td>\n",
       "      <td>1.157254</td>\n",
       "      <td>1.161105</td>\n",
       "      <td>1.164912</td>\n",
       "      <td>1.168669</td>\n",
       "      <td>1.172367</td>\n",
       "      <td>1.176004</td>\n",
       "      <td>1.179562</td>\n",
       "      <td>1.183029</td>\n",
       "      <td>1.186393</td>\n",
       "      <td>1.189638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4832838 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         feat_0    feat_1      feat_2      feat_3      feat_4      feat_5  \\\n",
       "0         -64.0 -180.0000   98848.380   99195.690   99320.914   98469.160   \n",
       "1         -64.0 -177.1875   98898.160   99224.910   99322.336   98480.360   \n",
       "2         -64.0 -174.3750   98945.240   99245.610   99325.310   98495.400   \n",
       "3         -64.0 -171.5625   98972.610   99259.370   99331.414   98527.730   \n",
       "4         -64.0 -168.7500   98969.490   99280.640   99335.805   98585.020   \n",
       "...         ...       ...         ...         ...         ...         ...   \n",
       "4832833    62.0   -5.6250  100949.125  101105.414   99110.170  100612.875   \n",
       "4832834    62.0   -2.8125  101014.790  101204.120   99232.990  100601.810   \n",
       "4832835    62.0    0.0000  101065.984  101295.586   99371.120  100607.914   \n",
       "4832836    62.0    2.8125  101128.520  101377.560   99537.330  100660.810   \n",
       "4832837    62.0   19.6875  101012.664  101396.560  100673.860  101356.150   \n",
       "\n",
       "             feat_6      feat_7      feat_8      feat_9  ...  add_feat_14  \\\n",
       "0         98149.220   98269.195   98373.570   97659.090  ...     0.590394   \n",
       "1         98138.870   98256.570   98282.520   97621.800  ...     0.600649   \n",
       "2         98137.020   98236.720   98203.670   97581.520  ...     0.635043   \n",
       "3         98152.336   98207.220   98140.990   97533.445  ...     0.600441   \n",
       "4         98189.350   98166.090   98090.790   97491.330  ...     0.594315   \n",
       "...             ...         ...         ...         ...  ...          ...   \n",
       "4832833  101548.020  100916.230  101062.870  101320.266  ...     0.346133   \n",
       "4832834  101610.990  100828.586  101021.590  101323.670  ...     0.411259   \n",
       "4832835  101676.370  100745.620  100973.234  101342.250  ...     0.436968   \n",
       "4832836  101771.670  100720.375  100953.950  101417.695  ...     0.595102   \n",
       "4832837  102277.960  101128.240  100753.700  101534.125  ...     1.157254   \n",
       "\n",
       "         add_feat_15  add_feat_16  add_feat_17  add_feat_18  add_feat_19  \\\n",
       "0           0.578681     0.576274     0.548224     0.544279     0.540694   \n",
       "1           0.590663     0.582925     0.571722     0.569716     0.560472   \n",
       "2           0.606403     0.587164     0.555811     0.557459     0.557803   \n",
       "3           0.585508     0.584608     0.552524     0.554519     0.554806   \n",
       "4           0.586059     0.590216     0.585997     0.589768     0.594207   \n",
       "...              ...          ...          ...          ...          ...   \n",
       "4832833     0.347553     0.343245     0.345732     0.347781     0.350528   \n",
       "4832834     0.412691     0.409359     0.412484     0.412936     0.411689   \n",
       "4832835     0.423618     0.420498     0.409317     0.411599     0.408346   \n",
       "4832836     0.583280     0.582017     0.577438     0.579690     0.573577   \n",
       "4832837     1.161105     1.164912     1.168669     1.172367     1.176004   \n",
       "\n",
       "         add_feat_20  add_feat_21  add_feat_22  add_feat_23  \n",
       "0           0.538946     0.543125     0.545212     0.543470  \n",
       "1           0.563940     0.566644     0.561656     0.562393  \n",
       "2           0.561152     0.562297     0.552638     0.549103  \n",
       "3           0.555235     0.559270     0.558470     0.546810  \n",
       "4           0.576754     0.578006     0.574201     0.550796  \n",
       "...              ...          ...          ...          ...  \n",
       "4832833     0.353242     0.354397     0.357285     0.359468  \n",
       "4832834     0.414810     0.418158     0.421191     0.412557  \n",
       "4832835     0.408299     0.408404     0.408727     0.403374  \n",
       "4832836     0.575215     0.565446     0.560426     0.558835  \n",
       "4832837     1.179562     1.183029     1.186393     1.189638  \n",
       "\n",
       "[4832838 rows x 75 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "spanish-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "imported-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "oriented-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"max_depth\": 8,\n",
    "    'num_leaves': 255,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"n_estimators\": 2000,\n",
    "    #\"colsample_bytree\": 0.8, \n",
    "    #\"colsample_bynode\": 0.8,\n",
    "    \"verbose\": -1,\n",
    "    \"device\": \"gpu\",\n",
    "    \"gpu_device_id\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adverse-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_catb = dict(\n",
    "task_type=\"GPU\",\n",
    "devices='1',\n",
    "reg_lambda=0.438, learning_rate=0.1,\n",
    "max_depth=8, min_data_in_leaf=50,\n",
    "n_estimators=2000, verbose=50,\n",
    "objective='RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "affiliated-school",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_type': 'GPU',\n",
       " 'devices': '1',\n",
       " 'reg_lambda': 0.438,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 8,\n",
       " 'min_data_in_leaf': 50,\n",
       " 'n_estimators': 2000,\n",
       " 'verbose': 50,\n",
       " 'objective': 'RMSE'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_catb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "second-track",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: 3866270, Val: 966568\n",
      "\n",
      "\n",
      "Train: 3866270, Val: 966568\n",
      "\n",
      "\n",
      "Train: 3866270, Val: 966568\n",
      "\n",
      "\n",
      "Train: 3866271, Val: 966567\n",
      "\n",
      "\n",
      "Train: 3866271, Val: 966567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for state, (train_idx, val_idx) in enumerate(cv.split(data)):\n",
    "    X_train = data.drop(columns=[\"target\"]).iloc[train_idx].values\n",
    "    X_val = data.drop(columns=[\"target\"]).iloc[val_idx].values\n",
    "    y_train = data[\"target\"].iloc[train_idx].values\n",
    "    y_val = data[\"target\"].iloc[val_idx].values\n",
    "    print(f\"\\nTrain: {len(y_train)}, Val: {len(y_val)}\\n\")\n",
    "    lgb_model = lgb.LGBMRegressor(**params_lgb, random_state=state)\n",
    "    lgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)])\n",
    "    joblib.dump(lgb_model, f\"./trained3/lgb_model_fold{state}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "invisible-pennsylvania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: 3866270, Val: 966568\n",
      "\n",
      "0:\tlearn: 0.6049746\ttest: 0.6062935\tbest: 0.6062935 (0)\ttotal: 26.2ms\tremaining: 52.3s\n",
      "50:\tlearn: 0.5296514\ttest: 0.5318658\tbest: 0.5318658 (50)\ttotal: 1.15s\tremaining: 43.9s\n",
      "100:\tlearn: 0.5250806\ttest: 0.5277828\tbest: 0.5277828 (100)\ttotal: 2.29s\tremaining: 43.1s\n",
      "150:\tlearn: 0.5221449\ttest: 0.5252145\tbest: 0.5252145 (150)\ttotal: 3.45s\tremaining: 42.3s\n",
      "200:\tlearn: 0.5199432\ttest: 0.5233868\tbest: 0.5233868 (200)\ttotal: 4.62s\tremaining: 41.4s\n",
      "250:\tlearn: 0.5179033\ttest: 0.5217066\tbest: 0.5217066 (250)\ttotal: 5.78s\tremaining: 40.3s\n",
      "300:\tlearn: 0.5162292\ttest: 0.5204130\tbest: 0.5204130 (300)\ttotal: 6.92s\tremaining: 39s\n",
      "350:\tlearn: 0.5145869\ttest: 0.5191346\tbest: 0.5191346 (350)\ttotal: 8.07s\tremaining: 37.9s\n",
      "400:\tlearn: 0.5130627\ttest: 0.5179723\tbest: 0.5179723 (400)\ttotal: 9.24s\tremaining: 36.9s\n",
      "450:\tlearn: 0.5116078\ttest: 0.5168799\tbest: 0.5168799 (450)\ttotal: 10.4s\tremaining: 35.7s\n",
      "500:\tlearn: 0.5103458\ttest: 0.5159909\tbest: 0.5159909 (500)\ttotal: 11.5s\tremaining: 34.4s\n",
      "550:\tlearn: 0.5090960\ttest: 0.5151230\tbest: 0.5151230 (550)\ttotal: 12.6s\tremaining: 33.3s\n",
      "600:\tlearn: 0.5079431\ttest: 0.5142769\tbest: 0.5142769 (600)\ttotal: 13.8s\tremaining: 32.2s\n",
      "650:\tlearn: 0.5067979\ttest: 0.5134603\tbest: 0.5134603 (650)\ttotal: 15s\tremaining: 31s\n",
      "700:\tlearn: 0.5056728\ttest: 0.5127230\tbest: 0.5127230 (700)\ttotal: 16.1s\tremaining: 29.9s\n",
      "750:\tlearn: 0.5045429\ttest: 0.5119189\tbest: 0.5119189 (750)\ttotal: 17.3s\tremaining: 28.7s\n",
      "800:\tlearn: 0.5034256\ttest: 0.5111436\tbest: 0.5111436 (800)\ttotal: 18.4s\tremaining: 27.6s\n",
      "850:\tlearn: 0.5023832\ttest: 0.5103932\tbest: 0.5103932 (850)\ttotal: 19.6s\tremaining: 26.4s\n",
      "900:\tlearn: 0.5013882\ttest: 0.5097077\tbest: 0.5097077 (900)\ttotal: 20.7s\tremaining: 25.3s\n",
      "950:\tlearn: 0.5004402\ttest: 0.5090667\tbest: 0.5090667 (950)\ttotal: 21.9s\tremaining: 24.1s\n",
      "1000:\tlearn: 0.4994851\ttest: 0.5084396\tbest: 0.5084396 (1000)\ttotal: 23s\tremaining: 23s\n",
      "1050:\tlearn: 0.4985598\ttest: 0.5078605\tbest: 0.5078605 (1050)\ttotal: 24.2s\tremaining: 21.8s\n",
      "1100:\tlearn: 0.4977008\ttest: 0.5073299\tbest: 0.5073299 (1100)\ttotal: 25.4s\tremaining: 20.7s\n",
      "1150:\tlearn: 0.4968035\ttest: 0.5067699\tbest: 0.5067699 (1150)\ttotal: 26.5s\tremaining: 19.6s\n",
      "1200:\tlearn: 0.4959137\ttest: 0.5062158\tbest: 0.5062158 (1200)\ttotal: 27.7s\tremaining: 18.4s\n",
      "1250:\tlearn: 0.4950849\ttest: 0.5057274\tbest: 0.5057274 (1250)\ttotal: 28.8s\tremaining: 17.2s\n",
      "1300:\tlearn: 0.4942331\ttest: 0.5051653\tbest: 0.5051653 (1300)\ttotal: 29.9s\tremaining: 16.1s\n",
      "1350:\tlearn: 0.4934091\ttest: 0.5046309\tbest: 0.5046309 (1350)\ttotal: 31.1s\tremaining: 14.9s\n",
      "1400:\tlearn: 0.4925862\ttest: 0.5041442\tbest: 0.5041442 (1400)\ttotal: 32.3s\tremaining: 13.8s\n",
      "1450:\tlearn: 0.4917641\ttest: 0.5035986\tbest: 0.5035986 (1450)\ttotal: 33.4s\tremaining: 12.6s\n",
      "1500:\tlearn: 0.4909942\ttest: 0.5031268\tbest: 0.5031268 (1500)\ttotal: 34.6s\tremaining: 11.5s\n",
      "1550:\tlearn: 0.4902394\ttest: 0.5026931\tbest: 0.5026931 (1550)\ttotal: 35.8s\tremaining: 10.3s\n",
      "1600:\tlearn: 0.4894738\ttest: 0.5022300\tbest: 0.5022300 (1600)\ttotal: 36.9s\tremaining: 9.2s\n",
      "1650:\tlearn: 0.4886997\ttest: 0.5017964\tbest: 0.5017964 (1650)\ttotal: 38.1s\tremaining: 8.05s\n",
      "1700:\tlearn: 0.4879853\ttest: 0.5013902\tbest: 0.5013902 (1700)\ttotal: 39.2s\tremaining: 6.89s\n",
      "1750:\tlearn: 0.4872637\ttest: 0.5009435\tbest: 0.5009435 (1750)\ttotal: 40.3s\tremaining: 5.74s\n",
      "1800:\tlearn: 0.4865479\ttest: 0.5005584\tbest: 0.5005584 (1800)\ttotal: 41.5s\tremaining: 4.59s\n",
      "1850:\tlearn: 0.4858727\ttest: 0.5001393\tbest: 0.5001393 (1850)\ttotal: 42.7s\tremaining: 3.43s\n",
      "1900:\tlearn: 0.4851709\ttest: 0.4997018\tbest: 0.4997018 (1900)\ttotal: 43.8s\tremaining: 2.28s\n",
      "1950:\tlearn: 0.4844687\ttest: 0.4992854\tbest: 0.4992854 (1950)\ttotal: 45s\tremaining: 1.13s\n",
      "1999:\tlearn: 0.4838156\ttest: 0.4988742\tbest: 0.4988742 (1999)\ttotal: 46.1s\tremaining: 0us\n",
      "bestTest = 0.4988741997\n",
      "bestIteration = 1999\n",
      "\n",
      "Train: 3866270, Val: 966568\n",
      "\n",
      "0:\tlearn: 0.6052699\ttest: 0.6051610\tbest: 0.6051610 (0)\ttotal: 22.3ms\tremaining: 44.6s\n",
      "50:\tlearn: 0.5301179\ttest: 0.5302121\tbest: 0.5302121 (50)\ttotal: 1.14s\tremaining: 43.7s\n",
      "100:\tlearn: 0.5253448\ttest: 0.5258535\tbest: 0.5258535 (100)\ttotal: 2.3s\tremaining: 43.3s\n",
      "150:\tlearn: 0.5225840\ttest: 0.5235837\tbest: 0.5235837 (150)\ttotal: 3.43s\tremaining: 42s\n",
      "200:\tlearn: 0.5202330\ttest: 0.5217143\tbest: 0.5217143 (200)\ttotal: 4.58s\tremaining: 41s\n",
      "250:\tlearn: 0.5182322\ttest: 0.5201187\tbest: 0.5201187 (250)\ttotal: 5.72s\tremaining: 39.8s\n",
      "300:\tlearn: 0.5165460\ttest: 0.5188189\tbest: 0.5188189 (300)\ttotal: 6.88s\tremaining: 38.8s\n",
      "350:\tlearn: 0.5149411\ttest: 0.5175854\tbest: 0.5175854 (350)\ttotal: 8.04s\tremaining: 37.8s\n",
      "400:\tlearn: 0.5134977\ttest: 0.5165298\tbest: 0.5165298 (400)\ttotal: 9.2s\tremaining: 36.7s\n",
      "450:\tlearn: 0.5120603\ttest: 0.5155098\tbest: 0.5155098 (450)\ttotal: 10.4s\tremaining: 35.6s\n",
      "500:\tlearn: 0.5107383\ttest: 0.5146338\tbest: 0.5146338 (500)\ttotal: 11.5s\tremaining: 34.5s\n",
      "550:\tlearn: 0.5094386\ttest: 0.5137479\tbest: 0.5137479 (550)\ttotal: 12.7s\tremaining: 33.4s\n",
      "600:\tlearn: 0.5082327\ttest: 0.5129140\tbest: 0.5129140 (600)\ttotal: 13.8s\tremaining: 32.1s\n",
      "650:\tlearn: 0.5070482\ttest: 0.5121153\tbest: 0.5121153 (650)\ttotal: 15s\tremaining: 31s\n",
      "700:\tlearn: 0.5059865\ttest: 0.5113960\tbest: 0.5113960 (700)\ttotal: 16.1s\tremaining: 29.8s\n",
      "750:\tlearn: 0.5048519\ttest: 0.5105912\tbest: 0.5105912 (750)\ttotal: 17.2s\tremaining: 28.6s\n",
      "800:\tlearn: 0.5037645\ttest: 0.5098566\tbest: 0.5098566 (800)\ttotal: 18.4s\tremaining: 27.5s\n",
      "850:\tlearn: 0.5027766\ttest: 0.5092211\tbest: 0.5092211 (850)\ttotal: 19.5s\tremaining: 26.3s\n",
      "900:\tlearn: 0.5017957\ttest: 0.5085906\tbest: 0.5085906 (900)\ttotal: 20.7s\tremaining: 25.2s\n",
      "950:\tlearn: 0.5008253\ttest: 0.5079677\tbest: 0.5079677 (950)\ttotal: 21.8s\tremaining: 24.1s\n",
      "1000:\tlearn: 0.4998165\ttest: 0.5073325\tbest: 0.5073325 (1000)\ttotal: 23s\tremaining: 22.9s\n",
      "1050:\tlearn: 0.4988930\ttest: 0.5067378\tbest: 0.5067378 (1050)\ttotal: 24.1s\tremaining: 21.8s\n",
      "1100:\tlearn: 0.4979925\ttest: 0.5061303\tbest: 0.5061303 (1100)\ttotal: 25.2s\tremaining: 20.6s\n",
      "1150:\tlearn: 0.4970902\ttest: 0.5055205\tbest: 0.5055205 (1150)\ttotal: 26.4s\tremaining: 19.5s\n",
      "1200:\tlearn: 0.4962154\ttest: 0.5049885\tbest: 0.5049885 (1200)\ttotal: 27.5s\tremaining: 18.3s\n",
      "1250:\tlearn: 0.4953534\ttest: 0.5044684\tbest: 0.5044684 (1250)\ttotal: 28.7s\tremaining: 17.2s\n",
      "1300:\tlearn: 0.4945568\ttest: 0.5040041\tbest: 0.5040041 (1300)\ttotal: 29.8s\tremaining: 16s\n",
      "1350:\tlearn: 0.4937721\ttest: 0.5035105\tbest: 0.5035105 (1350)\ttotal: 31s\tremaining: 14.9s\n",
      "1400:\tlearn: 0.4929686\ttest: 0.5030086\tbest: 0.5030086 (1400)\ttotal: 32.1s\tremaining: 13.7s\n",
      "1450:\tlearn: 0.4921788\ttest: 0.5024840\tbest: 0.5024840 (1450)\ttotal: 33.3s\tremaining: 12.6s\n",
      "1500:\tlearn: 0.4914498\ttest: 0.5020434\tbest: 0.5020434 (1500)\ttotal: 34.4s\tremaining: 11.4s\n",
      "1550:\tlearn: 0.4906696\ttest: 0.5015780\tbest: 0.5015780 (1550)\ttotal: 35.6s\tremaining: 10.3s\n",
      "1600:\tlearn: 0.4899061\ttest: 0.5011001\tbest: 0.5011001 (1600)\ttotal: 36.7s\tremaining: 9.15s\n",
      "1650:\tlearn: 0.4891252\ttest: 0.5006307\tbest: 0.5006307 (1650)\ttotal: 37.9s\tremaining: 8s\n",
      "1700:\tlearn: 0.4883924\ttest: 0.5001923\tbest: 0.5001923 (1700)\ttotal: 39s\tremaining: 6.85s\n",
      "1750:\tlearn: 0.4876599\ttest: 0.4997720\tbest: 0.4997720 (1750)\ttotal: 40.2s\tremaining: 5.71s\n",
      "1800:\tlearn: 0.4869012\ttest: 0.4993082\tbest: 0.4993082 (1800)\ttotal: 41.3s\tremaining: 4.57s\n",
      "1850:\tlearn: 0.4862107\ttest: 0.4989331\tbest: 0.4989331 (1850)\ttotal: 42.5s\tremaining: 3.42s\n",
      "1900:\tlearn: 0.4854946\ttest: 0.4984729\tbest: 0.4984729 (1900)\ttotal: 43.6s\tremaining: 2.27s\n",
      "1950:\tlearn: 0.4847966\ttest: 0.4980558\tbest: 0.4980558 (1950)\ttotal: 44.8s\tremaining: 1.12s\n",
      "1999:\tlearn: 0.4841112\ttest: 0.4976582\tbest: 0.4976582 (1999)\ttotal: 45.9s\tremaining: 0us\n",
      "bestTest = 0.4976582053\n",
      "bestIteration = 1999\n",
      "\n",
      "Train: 3866270, Val: 966568\n",
      "\n",
      "0:\tlearn: 0.6050988\ttest: 0.6057188\tbest: 0.6057188 (0)\ttotal: 22ms\tremaining: 44s\n",
      "50:\tlearn: 0.5298489\ttest: 0.5307634\tbest: 0.5307634 (50)\ttotal: 1.14s\tremaining: 43.7s\n",
      "100:\tlearn: 0.5253403\ttest: 0.5266489\tbest: 0.5266489 (100)\ttotal: 2.31s\tremaining: 43.4s\n",
      "150:\tlearn: 0.5223940\ttest: 0.5240832\tbest: 0.5240832 (150)\ttotal: 3.46s\tremaining: 42.4s\n",
      "200:\tlearn: 0.5199922\ttest: 0.5220671\tbest: 0.5220671 (200)\ttotal: 4.63s\tremaining: 41.4s\n",
      "250:\tlearn: 0.5181680\ttest: 0.5206642\tbest: 0.5206642 (250)\ttotal: 5.76s\tremaining: 40.1s\n",
      "300:\tlearn: 0.5164445\ttest: 0.5192569\tbest: 0.5192569 (300)\ttotal: 6.92s\tremaining: 39.1s\n",
      "350:\tlearn: 0.5148721\ttest: 0.5180851\tbest: 0.5180851 (350)\ttotal: 8.1s\tremaining: 38.1s\n",
      "400:\tlearn: 0.5132975\ttest: 0.5168887\tbest: 0.5168887 (400)\ttotal: 9.26s\tremaining: 36.9s\n",
      "450:\tlearn: 0.5119736\ttest: 0.5159208\tbest: 0.5159208 (450)\ttotal: 10.4s\tremaining: 35.8s\n",
      "500:\tlearn: 0.5105850\ttest: 0.5149392\tbest: 0.5149392 (500)\ttotal: 11.6s\tremaining: 34.6s\n",
      "550:\tlearn: 0.5093730\ttest: 0.5140745\tbest: 0.5140745 (550)\ttotal: 12.7s\tremaining: 33.4s\n",
      "600:\tlearn: 0.5081404\ttest: 0.5132173\tbest: 0.5132173 (600)\ttotal: 13.9s\tremaining: 32.3s\n",
      "650:\tlearn: 0.5069418\ttest: 0.5123740\tbest: 0.5123740 (650)\ttotal: 15s\tremaining: 31.1s\n",
      "700:\tlearn: 0.5058085\ttest: 0.5116091\tbest: 0.5116091 (700)\ttotal: 16.2s\tremaining: 30s\n",
      "750:\tlearn: 0.5046859\ttest: 0.5108463\tbest: 0.5108463 (750)\ttotal: 17.3s\tremaining: 28.8s\n",
      "800:\tlearn: 0.5035861\ttest: 0.5100893\tbest: 0.5100893 (800)\ttotal: 18.5s\tremaining: 27.7s\n",
      "850:\tlearn: 0.5025456\ttest: 0.5094400\tbest: 0.5094400 (850)\ttotal: 19.6s\tremaining: 26.5s\n",
      "900:\tlearn: 0.5015478\ttest: 0.5087877\tbest: 0.5087877 (900)\ttotal: 20.8s\tremaining: 25.4s\n",
      "950:\tlearn: 0.5005427\ttest: 0.5081188\tbest: 0.5081188 (950)\ttotal: 21.9s\tremaining: 24.2s\n",
      "1000:\tlearn: 0.4996085\ttest: 0.5074981\tbest: 0.5074981 (1000)\ttotal: 23.1s\tremaining: 23s\n",
      "1050:\tlearn: 0.4986252\ttest: 0.5068596\tbest: 0.5068596 (1050)\ttotal: 24.2s\tremaining: 21.9s\n",
      "1100:\tlearn: 0.4976671\ttest: 0.5062275\tbest: 0.5062275 (1100)\ttotal: 25.3s\tremaining: 20.7s\n",
      "1150:\tlearn: 0.4967282\ttest: 0.5055821\tbest: 0.5055821 (1150)\ttotal: 26.5s\tremaining: 19.5s\n",
      "1200:\tlearn: 0.4958628\ttest: 0.5050398\tbest: 0.5050398 (1200)\ttotal: 27.7s\tremaining: 18.4s\n",
      "1250:\tlearn: 0.4950253\ttest: 0.5045414\tbest: 0.5045414 (1250)\ttotal: 28.8s\tremaining: 17.2s\n",
      "1300:\tlearn: 0.4941629\ttest: 0.5039822\tbest: 0.5039822 (1300)\ttotal: 30s\tremaining: 16.1s\n",
      "1350:\tlearn: 0.4933599\ttest: 0.5035117\tbest: 0.5035117 (1350)\ttotal: 31.1s\tremaining: 14.9s\n",
      "1400:\tlearn: 0.4925544\ttest: 0.5030102\tbest: 0.5030102 (1400)\ttotal: 32.3s\tremaining: 13.8s\n",
      "1450:\tlearn: 0.4917250\ttest: 0.5024777\tbest: 0.5024777 (1450)\ttotal: 33.4s\tremaining: 12.7s\n",
      "1500:\tlearn: 0.4909360\ttest: 0.5019889\tbest: 0.5019889 (1500)\ttotal: 34.6s\tremaining: 11.5s\n",
      "1550:\tlearn: 0.4901720\ttest: 0.5015449\tbest: 0.5015449 (1550)\ttotal: 35.8s\tremaining: 10.3s\n",
      "1600:\tlearn: 0.4893782\ttest: 0.5010437\tbest: 0.5010437 (1600)\ttotal: 36.9s\tremaining: 9.2s\n",
      "1650:\tlearn: 0.4886072\ttest: 0.5005668\tbest: 0.5005668 (1650)\ttotal: 38.1s\tremaining: 8.05s\n",
      "1700:\tlearn: 0.4878767\ttest: 0.5001249\tbest: 0.5001249 (1700)\ttotal: 39.2s\tremaining: 6.89s\n",
      "1750:\tlearn: 0.4871813\ttest: 0.4997256\tbest: 0.4997256 (1750)\ttotal: 40.4s\tremaining: 5.74s\n",
      "1800:\tlearn: 0.4864851\ttest: 0.4993074\tbest: 0.4993074 (1800)\ttotal: 41.6s\tremaining: 4.59s\n",
      "1850:\tlearn: 0.4857787\ttest: 0.4989017\tbest: 0.4989017 (1850)\ttotal: 42.7s\tremaining: 3.44s\n",
      "1900:\tlearn: 0.4850657\ttest: 0.4984565\tbest: 0.4984565 (1900)\ttotal: 43.9s\tremaining: 2.28s\n",
      "1950:\tlearn: 0.4843760\ttest: 0.4980447\tbest: 0.4980447 (1950)\ttotal: 45s\tremaining: 1.13s\n",
      "1999:\tlearn: 0.4837119\ttest: 0.4976777\tbest: 0.4976777 (1999)\ttotal: 46.2s\tremaining: 0us\n",
      "bestTest = 0.4976776785\n",
      "bestIteration = 1999\n",
      "\n",
      "Train: 3866271, Val: 966567\n",
      "\n",
      "0:\tlearn: 0.6050709\ttest: 0.6057140\tbest: 0.6057140 (0)\ttotal: 22.6ms\tremaining: 45.2s\n",
      "50:\tlearn: 0.5300445\ttest: 0.5306134\tbest: 0.5306134 (50)\ttotal: 1.13s\tremaining: 43s\n",
      "100:\tlearn: 0.5254313\ttest: 0.5264081\tbest: 0.5264081 (100)\ttotal: 2.28s\tremaining: 42.9s\n",
      "150:\tlearn: 0.5224123\ttest: 0.5238516\tbest: 0.5238516 (150)\ttotal: 3.42s\tremaining: 41.9s\n",
      "200:\tlearn: 0.5202163\ttest: 0.5220131\tbest: 0.5220131 (200)\ttotal: 4.56s\tremaining: 40.8s\n",
      "250:\tlearn: 0.5181930\ttest: 0.5204649\tbest: 0.5204649 (250)\ttotal: 5.72s\tremaining: 39.8s\n",
      "300:\tlearn: 0.5164045\ttest: 0.5191069\tbest: 0.5191069 (300)\ttotal: 6.91s\tremaining: 39s\n",
      "350:\tlearn: 0.5148131\ttest: 0.5178501\tbest: 0.5178501 (350)\ttotal: 8.04s\tremaining: 37.8s\n",
      "400:\tlearn: 0.5133477\ttest: 0.5167907\tbest: 0.5167907 (400)\ttotal: 9.19s\tremaining: 36.7s\n",
      "450:\tlearn: 0.5119458\ttest: 0.5157470\tbest: 0.5157470 (450)\ttotal: 10.3s\tremaining: 35.5s\n",
      "500:\tlearn: 0.5106409\ttest: 0.5148160\tbest: 0.5148160 (500)\ttotal: 11.5s\tremaining: 34.4s\n",
      "550:\tlearn: 0.5093173\ttest: 0.5139006\tbest: 0.5139006 (550)\ttotal: 12.7s\tremaining: 33.3s\n",
      "600:\tlearn: 0.5081268\ttest: 0.5130678\tbest: 0.5130678 (600)\ttotal: 13.8s\tremaining: 32.2s\n",
      "650:\tlearn: 0.5069234\ttest: 0.5122366\tbest: 0.5122366 (650)\ttotal: 15s\tremaining: 31s\n",
      "700:\tlearn: 0.5058662\ttest: 0.5115040\tbest: 0.5115040 (700)\ttotal: 16.1s\tremaining: 29.9s\n",
      "750:\tlearn: 0.5047871\ttest: 0.5107657\tbest: 0.5107657 (750)\ttotal: 17.3s\tremaining: 28.7s\n",
      "800:\tlearn: 0.5037253\ttest: 0.5100547\tbest: 0.5100547 (800)\ttotal: 18.4s\tremaining: 27.6s\n",
      "850:\tlearn: 0.5027567\ttest: 0.5094288\tbest: 0.5094288 (850)\ttotal: 19.6s\tremaining: 26.4s\n",
      "900:\tlearn: 0.5017270\ttest: 0.5087270\tbest: 0.5087270 (900)\ttotal: 20.7s\tremaining: 25.3s\n",
      "950:\tlearn: 0.5006586\ttest: 0.5080067\tbest: 0.5080067 (950)\ttotal: 21.9s\tremaining: 24.1s\n",
      "1000:\tlearn: 0.4997169\ttest: 0.5074057\tbest: 0.5074057 (1000)\ttotal: 23s\tremaining: 23s\n",
      "1050:\tlearn: 0.4987634\ttest: 0.5067928\tbest: 0.5067928 (1050)\ttotal: 24.2s\tremaining: 21.8s\n",
      "1100:\tlearn: 0.4979039\ttest: 0.5062598\tbest: 0.5062598 (1100)\ttotal: 25.3s\tremaining: 20.7s\n",
      "1150:\tlearn: 0.4969949\ttest: 0.5056739\tbest: 0.5056739 (1150)\ttotal: 26.5s\tremaining: 19.5s\n",
      "1200:\tlearn: 0.4961086\ttest: 0.5050922\tbest: 0.5050922 (1200)\ttotal: 27.6s\tremaining: 18.4s\n",
      "1250:\tlearn: 0.4952023\ttest: 0.5044911\tbest: 0.5044911 (1250)\ttotal: 28.8s\tremaining: 17.2s\n",
      "1300:\tlearn: 0.4943795\ttest: 0.5039982\tbest: 0.5039982 (1300)\ttotal: 29.9s\tremaining: 16.1s\n",
      "1350:\tlearn: 0.4935800\ttest: 0.5034925\tbest: 0.5034925 (1350)\ttotal: 31.1s\tremaining: 14.9s\n",
      "1400:\tlearn: 0.4927387\ttest: 0.5029835\tbest: 0.5029835 (1400)\ttotal: 32.2s\tremaining: 13.8s\n",
      "1450:\tlearn: 0.4919583\ttest: 0.5025248\tbest: 0.5025248 (1450)\ttotal: 33.4s\tremaining: 12.6s\n",
      "1500:\tlearn: 0.4912149\ttest: 0.5020502\tbest: 0.5020502 (1500)\ttotal: 34.5s\tremaining: 11.5s\n",
      "1550:\tlearn: 0.4904065\ttest: 0.5015535\tbest: 0.5015535 (1550)\ttotal: 35.7s\tremaining: 10.3s\n",
      "1600:\tlearn: 0.4896551\ttest: 0.5011244\tbest: 0.5011244 (1600)\ttotal: 36.9s\tremaining: 9.19s\n",
      "1650:\tlearn: 0.4888667\ttest: 0.5006249\tbest: 0.5006249 (1650)\ttotal: 38s\tremaining: 8.03s\n",
      "1700:\tlearn: 0.4881640\ttest: 0.5001924\tbest: 0.5001924 (1700)\ttotal: 39.2s\tremaining: 6.88s\n",
      "1750:\tlearn: 0.4874187\ttest: 0.4997243\tbest: 0.4997243 (1750)\ttotal: 40.3s\tremaining: 5.73s\n",
      "1800:\tlearn: 0.4866476\ttest: 0.4992397\tbest: 0.4992397 (1800)\ttotal: 41.4s\tremaining: 4.58s\n",
      "1850:\tlearn: 0.4858856\ttest: 0.4987596\tbest: 0.4987596 (1850)\ttotal: 42.6s\tremaining: 3.43s\n",
      "1900:\tlearn: 0.4851996\ttest: 0.4983510\tbest: 0.4983510 (1900)\ttotal: 43.7s\tremaining: 2.28s\n",
      "1950:\tlearn: 0.4845116\ttest: 0.4979637\tbest: 0.4979637 (1950)\ttotal: 44.9s\tremaining: 1.13s\n",
      "1999:\tlearn: 0.4838283\ttest: 0.4975522\tbest: 0.4975522 (1999)\ttotal: 46s\tremaining: 0us\n",
      "bestTest = 0.4975521994\n",
      "bestIteration = 1999\n",
      "\n",
      "Train: 3866271, Val: 966567\n",
      "\n",
      "0:\tlearn: 0.6056732\ttest: 0.6032419\tbest: 0.6032419 (0)\ttotal: 22.1ms\tremaining: 44.2s\n",
      "50:\tlearn: 0.5304555\ttest: 0.5287178\tbest: 0.5287178 (50)\ttotal: 1.14s\tremaining: 43.5s\n",
      "100:\tlearn: 0.5260057\ttest: 0.5247732\tbest: 0.5247732 (100)\ttotal: 2.27s\tremaining: 42.7s\n",
      "150:\tlearn: 0.5231346\ttest: 0.5223129\tbest: 0.5223129 (150)\ttotal: 3.42s\tremaining: 41.9s\n",
      "200:\tlearn: 0.5206769\ttest: 0.5202545\tbest: 0.5202545 (200)\ttotal: 4.58s\tremaining: 41s\n",
      "250:\tlearn: 0.5187746\ttest: 0.5187989\tbest: 0.5187989 (250)\ttotal: 5.74s\tremaining: 40s\n",
      "300:\tlearn: 0.5168862\ttest: 0.5173278\tbest: 0.5173278 (300)\ttotal: 6.9s\tremaining: 38.9s\n",
      "350:\tlearn: 0.5152977\ttest: 0.5161391\tbest: 0.5161391 (350)\ttotal: 8.07s\tremaining: 37.9s\n",
      "400:\tlearn: 0.5137542\ttest: 0.5150284\tbest: 0.5150284 (400)\ttotal: 9.21s\tremaining: 36.7s\n",
      "450:\tlearn: 0.5123544\ttest: 0.5140401\tbest: 0.5140401 (450)\ttotal: 10.4s\tremaining: 35.7s\n",
      "500:\tlearn: 0.5110235\ttest: 0.5130633\tbest: 0.5130633 (500)\ttotal: 11.5s\tremaining: 34.5s\n",
      "550:\tlearn: 0.5097666\ttest: 0.5121882\tbest: 0.5121882 (550)\ttotal: 12.7s\tremaining: 33.3s\n",
      "600:\tlearn: 0.5085778\ttest: 0.5113563\tbest: 0.5113563 (600)\ttotal: 13.8s\tremaining: 32.2s\n",
      "650:\tlearn: 0.5073754\ttest: 0.5105067\tbest: 0.5105067 (650)\ttotal: 15s\tremaining: 31.1s\n",
      "700:\tlearn: 0.5062541\ttest: 0.5097807\tbest: 0.5097807 (700)\ttotal: 16.1s\tremaining: 29.9s\n",
      "750:\tlearn: 0.5051069\ttest: 0.5089817\tbest: 0.5089817 (750)\ttotal: 17.3s\tremaining: 28.7s\n",
      "800:\tlearn: 0.5040357\ttest: 0.5082956\tbest: 0.5082956 (800)\ttotal: 18.4s\tremaining: 27.6s\n",
      "850:\tlearn: 0.5029806\ttest: 0.5075842\tbest: 0.5075842 (850)\ttotal: 19.6s\tremaining: 26.5s\n",
      "900:\tlearn: 0.5019940\ttest: 0.5069857\tbest: 0.5069857 (900)\ttotal: 20.7s\tremaining: 25.3s\n",
      "950:\tlearn: 0.5010189\ttest: 0.5063788\tbest: 0.5063788 (950)\ttotal: 21.9s\tremaining: 24.1s\n",
      "1000:\tlearn: 0.5000266\ttest: 0.5057338\tbest: 0.5057338 (1000)\ttotal: 23s\tremaining: 23s\n",
      "1050:\tlearn: 0.4990386\ttest: 0.5051371\tbest: 0.5051371 (1050)\ttotal: 24.2s\tremaining: 21.8s\n",
      "1100:\tlearn: 0.4981547\ttest: 0.5045653\tbest: 0.5045653 (1100)\ttotal: 25.3s\tremaining: 20.7s\n",
      "1150:\tlearn: 0.4973380\ttest: 0.5040924\tbest: 0.5040924 (1150)\ttotal: 26.5s\tremaining: 19.5s\n",
      "1200:\tlearn: 0.4964074\ttest: 0.5034610\tbest: 0.5034610 (1200)\ttotal: 27.6s\tremaining: 18.4s\n",
      "1250:\tlearn: 0.4955420\ttest: 0.5029172\tbest: 0.5029172 (1250)\ttotal: 28.8s\tremaining: 17.2s\n",
      "1300:\tlearn: 0.4947067\ttest: 0.5024185\tbest: 0.5024185 (1300)\ttotal: 29.9s\tremaining: 16.1s\n",
      "1350:\tlearn: 0.4938338\ttest: 0.5018943\tbest: 0.5018943 (1350)\ttotal: 31.1s\tremaining: 14.9s\n",
      "1400:\tlearn: 0.4930397\ttest: 0.5014153\tbest: 0.5014153 (1400)\ttotal: 32.2s\tremaining: 13.8s\n",
      "1450:\tlearn: 0.4922153\ttest: 0.5008968\tbest: 0.5008968 (1450)\ttotal: 33.3s\tremaining: 12.6s\n",
      "1500:\tlearn: 0.4914584\ttest: 0.5004361\tbest: 0.5004361 (1500)\ttotal: 34.5s\tremaining: 11.5s\n",
      "1550:\tlearn: 0.4906724\ttest: 0.4999622\tbest: 0.4999622 (1550)\ttotal: 35.6s\tremaining: 10.3s\n",
      "1600:\tlearn: 0.4898944\ttest: 0.4994999\tbest: 0.4994999 (1600)\ttotal: 36.8s\tremaining: 9.16s\n",
      "1650:\tlearn: 0.4890770\ttest: 0.4990093\tbest: 0.4990093 (1650)\ttotal: 37.9s\tremaining: 8.02s\n",
      "1700:\tlearn: 0.4883233\ttest: 0.4985115\tbest: 0.4985115 (1700)\ttotal: 39.1s\tremaining: 6.87s\n",
      "1750:\tlearn: 0.4876060\ttest: 0.4981081\tbest: 0.4981081 (1750)\ttotal: 40.2s\tremaining: 5.72s\n",
      "1800:\tlearn: 0.4869072\ttest: 0.4977010\tbest: 0.4977010 (1800)\ttotal: 41.4s\tremaining: 4.57s\n",
      "1850:\tlearn: 0.4862387\ttest: 0.4973252\tbest: 0.4973252 (1850)\ttotal: 42.5s\tremaining: 3.42s\n",
      "1900:\tlearn: 0.4855396\ttest: 0.4969244\tbest: 0.4969244 (1900)\ttotal: 43.7s\tremaining: 2.28s\n",
      "1950:\tlearn: 0.4848311\ttest: 0.4965174\tbest: 0.4965174 (1950)\ttotal: 44.9s\tremaining: 1.13s\n",
      "1999:\tlearn: 0.4841604\ttest: 0.4961415\tbest: 0.4961415 (1999)\ttotal: 46s\tremaining: 0us\n",
      "bestTest = 0.4961414502\n",
      "bestIteration = 1999\n"
     ]
    }
   ],
   "source": [
    "for state, (train_idx, val_idx) in enumerate(cv.split(data)):\n",
    "    X_train = data.drop(columns=[\"target\"]).iloc[train_idx].values\n",
    "    X_val = data.drop(columns=[\"target\"]).iloc[val_idx].values\n",
    "    y_train = data[\"target\"].iloc[train_idx].values\n",
    "    y_val = data[\"target\"].iloc[val_idx].values\n",
    "    print(f\"\\nTrain: {len(y_train)}, Val: {len(y_val)}\\n\")\n",
    "    catb_model = cat.CatBoostRegressor(**params_catb, random_state=state)\n",
    "    catb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    early_stopping_rounds=200)\n",
    "    joblib.dump(catb_model, f\"./trained3/catb_model_fold{state}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-wales",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecmldeep",
   "language": "python",
   "name": "ecmldeep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
