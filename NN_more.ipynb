{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "textile-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faced-london",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "plastic-hygiene",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "electoral-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything():\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    print('-----Seed Set!-----') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lyric-aviation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Seed Set!-----\n"
     ]
    }
   ],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "referenced-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_merge_data(data_type=\"training\"):\n",
    "    #\n",
    "    Dfs = []\n",
    "    train_df = []\n",
    "    train_output = pd.read_csv(\"./data_phase_one/training_output.csv\", header=None)\n",
    "    coords = pd.read_csv(\"data_phase_one/coords.csv\", header=None)\n",
    "    for name in [\"mslp\", \"sst\", \"ssta\", \"t2m\"]:\n",
    "        df = pd.read_csv(f\"./data_phase_one/{data_type}_input_{name}.csv\", header=None)\n",
    "        print(\"Shape df\", df.shape)\n",
    "        Dfs.append(df)\n",
    "    j = 0\n",
    "    for i in range(0, df.shape[0]-12, 1):\n",
    "        feat = pd.concat([coords, Dfs[0].iloc[i:i+12], Dfs[1].iloc[i:i+12], Dfs[2].iloc[i:i+12], Dfs[3].iloc[i:i+12]], axis=0).reset_index(drop=True)\n",
    "        tgt = train_output.iloc[i+11]\n",
    "        tgt = tgt.T.reset_index(drop=True)\n",
    "        feat = feat.T.reset_index(drop=True)\n",
    "        feat = pd.concat([feat, tgt], axis=1)\n",
    "        assert feat.shape[1]==51\n",
    "        feat.columns = [f\"feat_{k}\" for k in range(feat.shape[1]-1)] + [\"target\"]\n",
    "        if j < 3:\n",
    "            print(feat.columns)\n",
    "        train_df.append(feat)\n",
    "        j += 1\n",
    "    train_df = pd.concat(train_df, axis=0)\n",
    "    #Dfs.index = [f\"col_{i}\" for i in range(848)]\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alert-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_csv(\"data_phase_one/coords.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "consolidated-philippines",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape df (849, 5774)\n",
      "Shape df (849, 5774)\n",
      "Shape df (849, 5774)\n",
      "Shape df (849, 5774)\n",
      "Index(['feat_0', 'feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6',\n",
      "       'feat_7', 'feat_8', 'feat_9', 'feat_10', 'feat_11', 'feat_12',\n",
      "       'feat_13', 'feat_14', 'feat_15', 'feat_16', 'feat_17', 'feat_18',\n",
      "       'feat_19', 'feat_20', 'feat_21', 'feat_22', 'feat_23', 'feat_24',\n",
      "       'feat_25', 'feat_26', 'feat_27', 'feat_28', 'feat_29', 'feat_30',\n",
      "       'feat_31', 'feat_32', 'feat_33', 'feat_34', 'feat_35', 'feat_36',\n",
      "       'feat_37', 'feat_38', 'feat_39', 'feat_40', 'feat_41', 'feat_42',\n",
      "       'feat_43', 'feat_44', 'feat_45', 'feat_46', 'feat_47', 'feat_48',\n",
      "       'feat_49', 'target'],\n",
      "      dtype='object')\n",
      "Index(['feat_0', 'feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6',\n",
      "       'feat_7', 'feat_8', 'feat_9', 'feat_10', 'feat_11', 'feat_12',\n",
      "       'feat_13', 'feat_14', 'feat_15', 'feat_16', 'feat_17', 'feat_18',\n",
      "       'feat_19', 'feat_20', 'feat_21', 'feat_22', 'feat_23', 'feat_24',\n",
      "       'feat_25', 'feat_26', 'feat_27', 'feat_28', 'feat_29', 'feat_30',\n",
      "       'feat_31', 'feat_32', 'feat_33', 'feat_34', 'feat_35', 'feat_36',\n",
      "       'feat_37', 'feat_38', 'feat_39', 'feat_40', 'feat_41', 'feat_42',\n",
      "       'feat_43', 'feat_44', 'feat_45', 'feat_46', 'feat_47', 'feat_48',\n",
      "       'feat_49', 'target'],\n",
      "      dtype='object')\n",
      "Index(['feat_0', 'feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6',\n",
      "       'feat_7', 'feat_8', 'feat_9', 'feat_10', 'feat_11', 'feat_12',\n",
      "       'feat_13', 'feat_14', 'feat_15', 'feat_16', 'feat_17', 'feat_18',\n",
      "       'feat_19', 'feat_20', 'feat_21', 'feat_22', 'feat_23', 'feat_24',\n",
      "       'feat_25', 'feat_26', 'feat_27', 'feat_28', 'feat_29', 'feat_30',\n",
      "       'feat_31', 'feat_32', 'feat_33', 'feat_34', 'feat_35', 'feat_36',\n",
      "       'feat_37', 'feat_38', 'feat_39', 'feat_40', 'feat_41', 'feat_42',\n",
      "       'feat_43', 'feat_44', 'feat_45', 'feat_46', 'feat_47', 'feat_48',\n",
      "       'feat_49', 'target'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = read_merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "alpha-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = train_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "romance-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_stats = np.load(\"monthly_stats.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adaptive-curtis",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_stats = np.tile(monthly_stats,(len(data)//5774,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "subject-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_stats_df = pd.DataFrame(monthly_stats, columns=[f\"add_feat_{i}\" for i in range(24)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "organized-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data.reset_index(drop=True), monthly_stats_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-eclipse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "northern-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "liked-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "isolated-lending",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=n_splits, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fitted-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dims = {'linear': 128, 'input_shape': [2,37]}\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, dims_dict):\n",
    "        super(GRU, self).__init__()\n",
    "        self.name = 'GRU'\n",
    "        self.dims_dict = dims_dict\n",
    "        self.gru = nn.GRU(dims_dict['input_shape'][1], 128, num_layers=1, batch_first=True)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(dims_dict['linear'], 1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh())\n",
    "        self.head = nn.Linear(128, 1)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        shape1, shape2 = self.dims_dict['input_shape']\n",
    "        x = x.reshape(x.shape[0],shape1,shape2)\n",
    "        if y is None:\n",
    "            _, hn = self.gru(x)\n",
    "            out = hn.reshape(hn.shape[1], -1)\n",
    "            out = self.head(self.linear(out))\n",
    "            return out\n",
    "        else:\n",
    "            _, hn = self.gru(x)\n",
    "            out = hn.reshape(hn.shape[1], -1)\n",
    "            #out = out.reshape(out.shape[0],-1)\n",
    "            #out = torch.cat([out, hn.reshape(hn.shape[1], -1)], dim=1)\n",
    "            out = self.head(self.linear(out))\n",
    "            loss = self.loss(out, y)\n",
    "            return loss\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, dims_dict):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.name = 'LSTM'\n",
    "        self.dims_dict = dims_dict\n",
    "        self.lstm = nn.LSTM(dims_dict['input_shape'][1], 128, num_layers=1, batch_first=True)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(dims_dict['linear'], 1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh())\n",
    "        self.head = nn.Linear(128, 1)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        shape1, shape2 = self.dims_dict['input_shape']\n",
    "        x = x.reshape(x.shape[0],shape1,shape2)\n",
    "        if y is None:\n",
    "            _, (hn, cn) = self.lstm(x)\n",
    "            #out = out.reshape(out.shape[0],-1)\n",
    "            #out = torch.cat([out, hn.reshape(hn.shape[1], -1)], dim=1)\n",
    "            out = hn.reshape(hn.shape[1], -1)\n",
    "            out = self.head(self.linear(out))\n",
    "            return out\n",
    "        else:\n",
    "            _, (hn, cn) = self.lstm(x)\n",
    "            #out = out.reshape(out.shape[0],-1)\n",
    "            #out = torch.cat([out, hn.reshape(hn.shape[1], -1)], dim=1)\n",
    "            out = hn.reshape(hn.shape[1], -1)\n",
    "            out = self.head(self.linear(out))\n",
    "            loss = self.loss(out, y)\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ready-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data_x, data_y=None):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.data_y is not None:\n",
    "            return self.data_x[idx], self.data_y[idx]\n",
    "        else:\n",
    "            return self.data_x[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "general-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(dataloader, model, opt, clip_norm):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for x, target in dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "            model.cuda()\n",
    "            x = x.cuda()\n",
    "            target = target.cuda()\n",
    "        loss = model(x, target)\n",
    "        train_losses.append(loss.item())\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), clip_norm)\n",
    "        opt.step()\n",
    "        train_losses.append(loss.item())\n",
    "    return np.mean(train_losses)\n",
    "\n",
    "def validation_step(dataloader, model):\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    for x, target in dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "            model.cuda()\n",
    "            x = x.cuda()\n",
    "            target = target.cuda()\n",
    "        loss = model(x,target)\n",
    "        val_losses.append(loss.item())\n",
    "    return np.mean(val_losses)\n",
    "\n",
    "\n",
    "def train_function(model, x_train, y_train, x_val, y_val, fold, epochs=20, clip_norm=3.0):\n",
    "    lr = 0.00035\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    data_x_train = torch.FloatTensor(x_train)\n",
    "    data_y_train = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "    data_x_val = torch.FloatTensor(x_val)\n",
    "    data_y_val = torch.FloatTensor(y_val).unsqueeze(1)\n",
    "    train_dataloader = DataLoader(Dataset(data_x_train, data_y_train), num_workers=4, batch_size=2048, shuffle=True)\n",
    "    val_dataloader = DataLoader(Dataset(data_x_val, data_y_val), num_workers=4, batch_size=2048, shuffle=False)\n",
    "    best_score = 1000\n",
    "    best_weights = None\n",
    "    early_stop = 0  \n",
    "    logs1 = open(f'./trained_nn2/logs_fold{fold}_GRU.txt', 'w')\n",
    "    logs2 = open(f'./trained_nn2/logs_fold{fold}_LSTM.txt', 'w')\n",
    "    for e in range(epochs):\n",
    "        loss = train_step(train_dataloader, model, opt, clip_norm)\n",
    "        val_loss = validation_step(val_dataloader, model)\n",
    "        early_stop += 1\n",
    "        if val_loss < best_score:\n",
    "            best_score = val_loss\n",
    "            best_weights = model.state_dict()\n",
    "            early_stop = 0\n",
    "            print('BEST ----> ')\n",
    "            if model.name == 'GRU':\n",
    "                logs1.write('BEST ----> \\n')\n",
    "            else:\n",
    "                logs2.write('BEST ----> \\n')\n",
    "        print(f\"{model.name} Epoch {e}, train_loss {round(loss,3)}, val_loss {round(val_loss, 3)}\")\n",
    "        if model.name == 'GRU':\n",
    "            logs1.write(f\"{model.name} Epoch {e}, train_loss {round(loss,3)}, val_loss {round(val_loss, 3)}\\n\")\n",
    "        else:\n",
    "            logs2.write(f\"{model.name} Epoch {e}, train_loss {round(loss,3)}, val_loss {round(val_loss, 3)}\\n\")\n",
    "        if early_stop >= 40:\n",
    "            print(\"!!! Early stopping\")\n",
    "            break\n",
    "    print(\"\\nLoading model best weights...\")\n",
    "    model.load_state_dict(best_weights)\n",
    "    print(\"\\nDone.\\n\")\n",
    "    logs1.close()\n",
    "    logs2.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dense-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_models(data, epochs=120, clip_norm=5.0):\n",
    "    trained_models = []\n",
    "    for i, (train_idx, val_idx) in enumerate(cv.split(data)):\n",
    "        print(f\"\\nSplit {i+1}/{n_splits}...\")\n",
    "        x_train = data.drop(columns=[\"target\"]).iloc[train_idx].values\n",
    "        x_val = data.drop(columns=[\"target\"]).iloc[val_idx].values\n",
    "        y_train = data[\"target\"].iloc[train_idx].values\n",
    "        y_val = data[\"target\"].iloc[val_idx].values\n",
    "        print(f\"\\nTrain: {y_train.shape}, Val: {y_val.shape})\\n\")\n",
    "        for Model in [GRU, LSTM]:\n",
    "            model = Model(hidden_dims)\n",
    "            model = train_function(model, x_train, y_train, x_val, y_val, i, epochs=epochs, clip_norm=clip_norm)\n",
    "            model.to('cpu')\n",
    "            torch.save(model.state_dict(), f'./trained_nn2/pytorch_{model.name}_fold{i}.pt')\n",
    "            trained_models.append(model)\n",
    "            torch.cuda.empty_cache()\n",
    "    return x_val, y_val, trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "intense-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_pytorch(model, dataloader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for x in dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "            model.cuda()\n",
    "            x = x.cuda()\n",
    "        pred = model(x).detach().cpu().numpy()\n",
    "        preds.append(pred)\n",
    "    model.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "    return np.concatenate(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "colored-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_prediction(X_test, trained_models):\n",
    "    all_preds = []\n",
    "    test_dataloader = DataLoader(Dataset(torch.FloatTensor(X_test)), num_workers=4, batch_size=4096, shuffle=False)\n",
    "    for i,model in enumerate(trained_models):\n",
    "        current_pred = inference_pytorch(model, test_dataloader)\n",
    "        all_preds.append(current_pred)\n",
    "    return np.stack(all_preds, axis=1).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-comfort",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split 1/5...\n",
      "\n",
      "Train: (3866270,), Val: (966568,))\n",
      "\n",
      "BEST ----> \n",
      "GRU Epoch 0, train_loss 0.318, val_loss 0.306\n",
      "GRU Epoch 1, train_loss 0.308, val_loss 0.326\n",
      "GRU Epoch 2, train_loss 0.307, val_loss 0.309\n",
      "GRU Epoch 3, train_loss 0.307, val_loss 0.312\n",
      "GRU Epoch 4, train_loss 0.307, val_loss 0.315\n",
      "GRU Epoch 5, train_loss 0.307, val_loss 0.319\n",
      "GRU Epoch 6, train_loss 0.306, val_loss 0.308\n",
      "GRU Epoch 7, train_loss 0.306, val_loss 0.318\n",
      "GRU Epoch 8, train_loss 0.306, val_loss 0.334\n",
      "GRU Epoch 9, train_loss 0.306, val_loss 0.342\n",
      "GRU Epoch 10, train_loss 0.306, val_loss 0.308\n",
      "GRU Epoch 11, train_loss 0.306, val_loss 0.323\n",
      "GRU Epoch 12, train_loss 0.306, val_loss 0.311\n",
      "GRU Epoch 13, train_loss 0.306, val_loss 0.414\n",
      "BEST ----> \n",
      "GRU Epoch 14, train_loss 0.306, val_loss 0.304\n",
      "GRU Epoch 15, train_loss 0.306, val_loss 0.309\n",
      "GRU Epoch 16, train_loss 0.306, val_loss 0.308\n",
      "GRU Epoch 17, train_loss 0.306, val_loss 0.332\n",
      "GRU Epoch 18, train_loss 0.306, val_loss 0.316\n",
      "GRU Epoch 19, train_loss 0.306, val_loss 0.311\n",
      "GRU Epoch 20, train_loss 0.306, val_loss 0.337\n",
      "GRU Epoch 21, train_loss 0.306, val_loss 0.32\n",
      "GRU Epoch 22, train_loss 0.307, val_loss 0.313\n",
      "GRU Epoch 23, train_loss 0.307, val_loss 0.341\n",
      "GRU Epoch 24, train_loss 0.306, val_loss 0.327\n",
      "GRU Epoch 25, train_loss 0.306, val_loss 0.318\n",
      "GRU Epoch 26, train_loss 0.306, val_loss 0.321\n",
      "GRU Epoch 27, train_loss 0.306, val_loss 0.317\n",
      "GRU Epoch 28, train_loss 0.306, val_loss 0.334\n",
      "GRU Epoch 29, train_loss 0.306, val_loss 0.311\n",
      "GRU Epoch 30, train_loss 0.305, val_loss 0.319\n",
      "GRU Epoch 31, train_loss 0.305, val_loss 0.358\n",
      "GRU Epoch 32, train_loss 0.305, val_loss 0.318\n",
      "GRU Epoch 33, train_loss 0.305, val_loss 0.359\n",
      "GRU Epoch 34, train_loss 0.305, val_loss 0.354\n",
      "GRU Epoch 35, train_loss 0.305, val_loss 0.547\n",
      "GRU Epoch 36, train_loss 0.305, val_loss 0.31\n",
      "GRU Epoch 37, train_loss 0.305, val_loss 0.323\n",
      "GRU Epoch 38, train_loss 0.305, val_loss 0.319\n",
      "GRU Epoch 39, train_loss 0.305, val_loss 0.307\n",
      "GRU Epoch 40, train_loss 0.305, val_loss 0.333\n",
      "GRU Epoch 41, train_loss 0.305, val_loss 0.326\n",
      "GRU Epoch 42, train_loss 0.305, val_loss 0.43\n",
      "GRU Epoch 43, train_loss 0.305, val_loss 0.406\n",
      "GRU Epoch 44, train_loss 0.305, val_loss 0.327\n",
      "GRU Epoch 45, train_loss 0.305, val_loss 0.405\n",
      "GRU Epoch 46, train_loss 0.305, val_loss 0.326\n",
      "GRU Epoch 47, train_loss 0.305, val_loss 0.39\n",
      "GRU Epoch 48, train_loss 0.305, val_loss 0.327\n",
      "GRU Epoch 49, train_loss 0.305, val_loss 0.309\n",
      "GRU Epoch 50, train_loss 0.305, val_loss 0.348\n",
      "GRU Epoch 51, train_loss 0.305, val_loss 0.384\n",
      "GRU Epoch 52, train_loss 0.305, val_loss 0.483\n",
      "GRU Epoch 53, train_loss 0.305, val_loss 0.32\n",
      "GRU Epoch 54, train_loss 0.305, val_loss 0.357\n",
      "!!! Early stopping\n",
      "\n",
      "Loading model best weights...\n",
      "\n",
      "Done.\n",
      "\n",
      "BEST ----> \n",
      "LSTM Epoch 0, train_loss 0.32, val_loss 0.349\n",
      "LSTM Epoch 1, train_loss 0.309, val_loss 0.376\n",
      "BEST ----> \n",
      "LSTM Epoch 2, train_loss 0.307, val_loss 0.306\n",
      "LSTM Epoch 3, train_loss 0.307, val_loss 0.406\n",
      "LSTM Epoch 4, train_loss 0.306, val_loss 0.31\n",
      "LSTM Epoch 5, train_loss 0.306, val_loss 0.373\n",
      "LSTM Epoch 6, train_loss 0.306, val_loss 0.325\n",
      "LSTM Epoch 7, train_loss 0.306, val_loss 0.308\n",
      "LSTM Epoch 8, train_loss 0.305, val_loss 0.307\n",
      "LSTM Epoch 9, train_loss 0.305, val_loss 0.32\n",
      "LSTM Epoch 10, train_loss 0.305, val_loss 0.319\n",
      "LSTM Epoch 11, train_loss 0.305, val_loss 0.321\n",
      "BEST ----> \n",
      "LSTM Epoch 12, train_loss 0.305, val_loss 0.305\n",
      "LSTM Epoch 13, train_loss 0.305, val_loss 0.327\n",
      "LSTM Epoch 14, train_loss 0.305, val_loss 0.323\n",
      "LSTM Epoch 15, train_loss 0.305, val_loss 0.309\n",
      "LSTM Epoch 16, train_loss 0.305, val_loss 0.355\n",
      "LSTM Epoch 17, train_loss 0.305, val_loss 0.305\n",
      "LSTM Epoch 18, train_loss 0.305, val_loss 0.324\n",
      "LSTM Epoch 19, train_loss 0.305, val_loss 0.335\n",
      "LSTM Epoch 20, train_loss 0.305, val_loss 0.314\n",
      "LSTM Epoch 21, train_loss 0.305, val_loss 0.326\n",
      "LSTM Epoch 22, train_loss 0.305, val_loss 0.327\n",
      "LSTM Epoch 23, train_loss 0.305, val_loss 0.327\n",
      "LSTM Epoch 24, train_loss 0.305, val_loss 0.311\n",
      "LSTM Epoch 25, train_loss 0.305, val_loss 0.332\n",
      "LSTM Epoch 26, train_loss 0.305, val_loss 0.319\n",
      "LSTM Epoch 27, train_loss 0.305, val_loss 0.322\n",
      "LSTM Epoch 28, train_loss 0.305, val_loss 0.371\n",
      "LSTM Epoch 29, train_loss 0.305, val_loss 0.31\n",
      "LSTM Epoch 30, train_loss 0.306, val_loss 0.346\n",
      "LSTM Epoch 31, train_loss 0.305, val_loss 0.305\n",
      "LSTM Epoch 32, train_loss 0.305, val_loss 0.319\n",
      "BEST ----> \n",
      "LSTM Epoch 33, train_loss 0.305, val_loss 0.304\n",
      "LSTM Epoch 34, train_loss 0.305, val_loss 0.375\n",
      "LSTM Epoch 35, train_loss 0.305, val_loss 0.317\n",
      "LSTM Epoch 36, train_loss 0.305, val_loss 0.31\n",
      "LSTM Epoch 37, train_loss 0.305, val_loss 0.332\n",
      "LSTM Epoch 38, train_loss 0.305, val_loss 0.404\n",
      "LSTM Epoch 39, train_loss 0.305, val_loss 0.327\n",
      "LSTM Epoch 40, train_loss 0.305, val_loss 0.331\n",
      "LSTM Epoch 41, train_loss 0.305, val_loss 0.321\n",
      "LSTM Epoch 42, train_loss 0.305, val_loss 0.358\n",
      "LSTM Epoch 43, train_loss 0.305, val_loss 0.352\n",
      "LSTM Epoch 44, train_loss 0.305, val_loss 0.33\n",
      "LSTM Epoch 45, train_loss 0.305, val_loss 0.311\n",
      "LSTM Epoch 46, train_loss 0.305, val_loss 0.384\n",
      "LSTM Epoch 47, train_loss 0.305, val_loss 0.318\n",
      "LSTM Epoch 48, train_loss 0.305, val_loss 0.411\n",
      "LSTM Epoch 49, train_loss 0.305, val_loss 0.415\n",
      "LSTM Epoch 50, train_loss 0.305, val_loss 0.408\n",
      "LSTM Epoch 51, train_loss 0.305, val_loss 0.311\n",
      "LSTM Epoch 52, train_loss 0.305, val_loss 0.308\n",
      "LSTM Epoch 53, train_loss 0.305, val_loss 0.348\n",
      "LSTM Epoch 54, train_loss 0.305, val_loss 0.338\n",
      "LSTM Epoch 55, train_loss 0.305, val_loss 0.307\n",
      "LSTM Epoch 56, train_loss 0.305, val_loss 0.305\n",
      "LSTM Epoch 57, train_loss 0.305, val_loss 0.314\n",
      "LSTM Epoch 58, train_loss 0.305, val_loss 0.307\n",
      "LSTM Epoch 59, train_loss 0.305, val_loss 0.313\n",
      "LSTM Epoch 60, train_loss 0.305, val_loss 0.342\n",
      "LSTM Epoch 61, train_loss 0.305, val_loss 0.407\n",
      "LSTM Epoch 62, train_loss 0.305, val_loss 0.338\n",
      "LSTM Epoch 63, train_loss 0.304, val_loss 0.329\n",
      "LSTM Epoch 64, train_loss 0.304, val_loss 0.316\n",
      "LSTM Epoch 65, train_loss 0.304, val_loss 0.307\n",
      "LSTM Epoch 66, train_loss 0.304, val_loss 0.324\n",
      "LSTM Epoch 67, train_loss 0.304, val_loss 0.32\n",
      "LSTM Epoch 68, train_loss 0.304, val_loss 0.311\n",
      "LSTM Epoch 69, train_loss 0.304, val_loss 0.305\n",
      "LSTM Epoch 70, train_loss 0.304, val_loss 0.325\n",
      "LSTM Epoch 71, train_loss 0.304, val_loss 0.387\n",
      "LSTM Epoch 72, train_loss 0.304, val_loss 0.31\n",
      "LSTM Epoch 73, train_loss 0.304, val_loss 0.323\n",
      "!!! Early stopping\n",
      "\n",
      "Loading model best weights...\n",
      "\n",
      "Done.\n",
      "\n",
      "\n",
      "Split 2/5...\n",
      "\n",
      "Train: (3866270,), Val: (966568,))\n",
      "\n",
      "BEST ----> \n",
      "GRU Epoch 0, train_loss 0.326, val_loss 0.326\n",
      "BEST ----> \n",
      "GRU Epoch 1, train_loss 0.313, val_loss 0.315\n",
      "GRU Epoch 2, train_loss 0.311, val_loss 0.333\n",
      "GRU Epoch 3, train_loss 0.31, val_loss 0.322\n",
      "BEST ----> \n",
      "GRU Epoch 4, train_loss 0.309, val_loss 0.313\n",
      "BEST ----> \n",
      "GRU Epoch 5, train_loss 0.308, val_loss 0.31\n",
      "GRU Epoch 6, train_loss 0.308, val_loss 0.315\n",
      "GRU Epoch 7, train_loss 0.308, val_loss 0.348\n",
      "GRU Epoch 8, train_loss 0.308, val_loss 0.335\n",
      "GRU Epoch 9, train_loss 0.307, val_loss 0.343\n",
      "GRU Epoch 10, train_loss 0.307, val_loss 0.312\n",
      "GRU Epoch 11, train_loss 0.307, val_loss 0.327\n",
      "GRU Epoch 12, train_loss 0.307, val_loss 0.342\n",
      "GRU Epoch 13, train_loss 0.307, val_loss 0.341\n",
      "GRU Epoch 14, train_loss 0.307, val_loss 0.337\n",
      "GRU Epoch 15, train_loss 0.307, val_loss 0.328\n",
      "GRU Epoch 16, train_loss 0.306, val_loss 0.324\n",
      "GRU Epoch 17, train_loss 0.306, val_loss 0.314\n",
      "GRU Epoch 18, train_loss 0.306, val_loss 0.318\n",
      "GRU Epoch 19, train_loss 0.306, val_loss 0.347\n",
      "GRU Epoch 20, train_loss 0.306, val_loss 0.331\n",
      "GRU Epoch 21, train_loss 0.306, val_loss 0.329\n",
      "GRU Epoch 22, train_loss 0.306, val_loss 0.369\n",
      "GRU Epoch 23, train_loss 0.306, val_loss 0.329\n",
      "GRU Epoch 24, train_loss 0.306, val_loss 0.348\n",
      "GRU Epoch 25, train_loss 0.306, val_loss 0.321\n",
      "GRU Epoch 26, train_loss 0.306, val_loss 0.325\n",
      "GRU Epoch 27, train_loss 0.306, val_loss 0.402\n",
      "GRU Epoch 28, train_loss 0.306, val_loss 0.328\n",
      "GRU Epoch 29, train_loss 0.306, val_loss 0.343\n",
      "GRU Epoch 30, train_loss 0.306, val_loss 0.364\n",
      "GRU Epoch 31, train_loss 0.306, val_loss 0.341\n",
      "GRU Epoch 32, train_loss 0.306, val_loss 0.326\n",
      "GRU Epoch 33, train_loss 0.306, val_loss 0.326\n",
      "GRU Epoch 34, train_loss 0.306, val_loss 0.321\n",
      "GRU Epoch 35, train_loss 0.306, val_loss 0.337\n",
      "GRU Epoch 36, train_loss 0.306, val_loss 0.322\n",
      "GRU Epoch 37, train_loss 0.306, val_loss 0.351\n",
      "GRU Epoch 38, train_loss 0.306, val_loss 0.32\n",
      "GRU Epoch 39, train_loss 0.306, val_loss 0.36\n",
      "GRU Epoch 40, train_loss 0.306, val_loss 0.371\n",
      "GRU Epoch 41, train_loss 0.306, val_loss 0.331\n",
      "GRU Epoch 42, train_loss 0.306, val_loss 0.339\n",
      "GRU Epoch 43, train_loss 0.306, val_loss 0.332\n",
      "GRU Epoch 44, train_loss 0.306, val_loss 0.336\n",
      "GRU Epoch 45, train_loss 0.306, val_loss 0.327\n",
      "!!! Early stopping\n",
      "\n",
      "Loading model best weights...\n",
      "\n",
      "Done.\n",
      "\n",
      "BEST ----> \n",
      "LSTM Epoch 0, train_loss 0.323, val_loss 0.364\n",
      "LSTM Epoch 1, train_loss 0.31, val_loss 0.38\n",
      "BEST ----> \n",
      "LSTM Epoch 2, train_loss 0.308, val_loss 0.308\n",
      "BEST ----> \n",
      "LSTM Epoch 3, train_loss 0.308, val_loss 0.308\n",
      "LSTM Epoch 4, train_loss 0.307, val_loss 0.548\n",
      "BEST ----> \n",
      "LSTM Epoch 5, train_loss 0.307, val_loss 0.308\n",
      "LSTM Epoch 6, train_loss 0.307, val_loss 0.317\n",
      "LSTM Epoch 7, train_loss 0.306, val_loss 0.316\n",
      "LSTM Epoch 8, train_loss 0.306, val_loss 0.316\n",
      "LSTM Epoch 9, train_loss 0.306, val_loss 0.317\n",
      "LSTM Epoch 10, train_loss 0.306, val_loss 0.326\n",
      "LSTM Epoch 11, train_loss 0.306, val_loss 0.338\n",
      "LSTM Epoch 12, train_loss 0.306, val_loss 0.311\n",
      "LSTM Epoch 13, train_loss 0.306, val_loss 0.308\n",
      "LSTM Epoch 14, train_loss 0.306, val_loss 0.317\n",
      "LSTM Epoch 15, train_loss 0.306, val_loss 0.308\n",
      "LSTM Epoch 16, train_loss 0.306, val_loss 0.311\n",
      "LSTM Epoch 17, train_loss 0.306, val_loss 0.371\n",
      "LSTM Epoch 18, train_loss 0.306, val_loss 0.327\n",
      "LSTM Epoch 19, train_loss 0.305, val_loss 0.322\n",
      "LSTM Epoch 20, train_loss 0.305, val_loss 0.313\n",
      "BEST ----> \n",
      "LSTM Epoch 21, train_loss 0.305, val_loss 0.307\n",
      "LSTM Epoch 22, train_loss 0.305, val_loss 0.309\n",
      "LSTM Epoch 23, train_loss 0.305, val_loss 0.315\n",
      "LSTM Epoch 24, train_loss 0.305, val_loss 0.321\n",
      "LSTM Epoch 25, train_loss 0.305, val_loss 0.316\n",
      "LSTM Epoch 26, train_loss 0.305, val_loss 0.317\n",
      "BEST ----> \n",
      "LSTM Epoch 27, train_loss 0.305, val_loss 0.306\n",
      "BEST ----> \n",
      "LSTM Epoch 28, train_loss 0.305, val_loss 0.305\n",
      "LSTM Epoch 29, train_loss 0.305, val_loss 0.325\n",
      "LSTM Epoch 30, train_loss 0.305, val_loss 0.378\n",
      "LSTM Epoch 31, train_loss 0.305, val_loss 0.312\n",
      "LSTM Epoch 32, train_loss 0.305, val_loss 0.34\n",
      "LSTM Epoch 33, train_loss 0.305, val_loss 0.314\n",
      "LSTM Epoch 34, train_loss 0.305, val_loss 0.317\n",
      "LSTM Epoch 35, train_loss 0.305, val_loss 0.316\n",
      "LSTM Epoch 36, train_loss 0.305, val_loss 0.305\n",
      "LSTM Epoch 37, train_loss 0.305, val_loss 0.32\n",
      "LSTM Epoch 38, train_loss 0.305, val_loss 0.309\n",
      "LSTM Epoch 39, train_loss 0.305, val_loss 0.328\n",
      "LSTM Epoch 40, train_loss 0.305, val_loss 0.308\n",
      "LSTM Epoch 41, train_loss 0.305, val_loss 0.307\n",
      "LSTM Epoch 42, train_loss 0.305, val_loss 0.309\n",
      "LSTM Epoch 43, train_loss 0.305, val_loss 0.31\n",
      "LSTM Epoch 44, train_loss 0.305, val_loss 0.334\n",
      "LSTM Epoch 45, train_loss 0.305, val_loss 0.311\n",
      "LSTM Epoch 46, train_loss 0.305, val_loss 0.345\n",
      "LSTM Epoch 47, train_loss 0.305, val_loss 0.324\n",
      "LSTM Epoch 48, train_loss 0.305, val_loss 0.369\n",
      "LSTM Epoch 49, train_loss 0.305, val_loss 0.31\n",
      "LSTM Epoch 50, train_loss 0.305, val_loss 0.31\n",
      "LSTM Epoch 51, train_loss 0.305, val_loss 0.335\n",
      "LSTM Epoch 52, train_loss 0.305, val_loss 0.307\n",
      "LSTM Epoch 53, train_loss 0.305, val_loss 0.315\n",
      "LSTM Epoch 54, train_loss 0.305, val_loss 0.365\n",
      "BEST ----> \n",
      "LSTM Epoch 55, train_loss 0.305, val_loss 0.305\n",
      "LSTM Epoch 56, train_loss 0.305, val_loss 0.311\n",
      "LSTM Epoch 57, train_loss 0.305, val_loss 0.308\n",
      "LSTM Epoch 58, train_loss 0.305, val_loss 0.308\n",
      "LSTM Epoch 59, train_loss 0.305, val_loss 0.313\n",
      "LSTM Epoch 60, train_loss 0.305, val_loss 0.34\n",
      "BEST ----> \n",
      "LSTM Epoch 61, train_loss 0.305, val_loss 0.305\n",
      "LSTM Epoch 62, train_loss 0.305, val_loss 0.307\n",
      "LSTM Epoch 63, train_loss 0.305, val_loss 0.31\n",
      "LSTM Epoch 64, train_loss 0.305, val_loss 0.341\n",
      "LSTM Epoch 65, train_loss 0.305, val_loss 0.308\n",
      "LSTM Epoch 66, train_loss 0.305, val_loss 0.306\n",
      "LSTM Epoch 67, train_loss 0.305, val_loss 0.306\n",
      "LSTM Epoch 68, train_loss 0.305, val_loss 0.308\n",
      "BEST ----> \n",
      "LSTM Epoch 69, train_loss 0.305, val_loss 0.305\n",
      "LSTM Epoch 70, train_loss 0.305, val_loss 0.318\n",
      "LSTM Epoch 71, train_loss 0.305, val_loss 0.322\n",
      "LSTM Epoch 72, train_loss 0.305, val_loss 0.308\n",
      "LSTM Epoch 73, train_loss 0.304, val_loss 0.307\n",
      "LSTM Epoch 74, train_loss 0.304, val_loss 0.308\n",
      "LSTM Epoch 75, train_loss 0.305, val_loss 0.318\n",
      "LSTM Epoch 76, train_loss 0.304, val_loss 0.311\n",
      "LSTM Epoch 77, train_loss 0.304, val_loss 0.313\n",
      "LSTM Epoch 78, train_loss 0.304, val_loss 0.306\n",
      "LSTM Epoch 79, train_loss 0.304, val_loss 0.313\n",
      "LSTM Epoch 80, train_loss 0.304, val_loss 0.307\n",
      "LSTM Epoch 81, train_loss 0.304, val_loss 0.306\n",
      "LSTM Epoch 82, train_loss 0.304, val_loss 0.404\n",
      "LSTM Epoch 83, train_loss 0.304, val_loss 0.317\n",
      "LSTM Epoch 84, train_loss 0.304, val_loss 0.391\n",
      "LSTM Epoch 85, train_loss 0.304, val_loss 0.31\n",
      "LSTM Epoch 86, train_loss 0.304, val_loss 0.319\n",
      "LSTM Epoch 87, train_loss 0.304, val_loss 0.31\n",
      "LSTM Epoch 88, train_loss 0.304, val_loss 0.315\n",
      "LSTM Epoch 89, train_loss 0.304, val_loss 0.306\n",
      "LSTM Epoch 90, train_loss 0.304, val_loss 0.332\n",
      "LSTM Epoch 91, train_loss 0.304, val_loss 0.308\n",
      "LSTM Epoch 92, train_loss 0.304, val_loss 0.346\n",
      "LSTM Epoch 93, train_loss 0.304, val_loss 0.37\n",
      "LSTM Epoch 94, train_loss 0.304, val_loss 0.311\n",
      "LSTM Epoch 95, train_loss 0.304, val_loss 0.308\n",
      "LSTM Epoch 96, train_loss 0.304, val_loss 0.31\n",
      "LSTM Epoch 97, train_loss 0.304, val_loss 0.323\n",
      "LSTM Epoch 98, train_loss 0.304, val_loss 0.364\n",
      "LSTM Epoch 99, train_loss 0.304, val_loss 0.308\n",
      "\n",
      "Loading model best weights...\n",
      "\n",
      "Done.\n",
      "\n",
      "\n",
      "Split 3/5...\n",
      "\n",
      "Train: (3866270,), Val: (966568,))\n",
      "\n",
      "BEST ----> \n",
      "GRU Epoch 0, train_loss 0.34, val_loss 0.314\n",
      "GRU Epoch 1, train_loss 0.326, val_loss 0.329\n",
      "GRU Epoch 2, train_loss 0.319, val_loss 0.323\n",
      "GRU Epoch 3, train_loss 0.313, val_loss 0.382\n",
      "GRU Epoch 4, train_loss 0.311, val_loss 0.383\n",
      "GRU Epoch 5, train_loss 0.31, val_loss 0.317\n",
      "GRU Epoch 6, train_loss 0.309, val_loss 0.324\n",
      "GRU Epoch 7, train_loss 0.309, val_loss 0.331\n",
      "GRU Epoch 8, train_loss 0.308, val_loss 0.351\n",
      "GRU Epoch 9, train_loss 0.308, val_loss 0.318\n",
      "GRU Epoch 10, train_loss 0.308, val_loss 0.354\n",
      "GRU Epoch 11, train_loss 0.308, val_loss 0.36\n",
      "GRU Epoch 12, train_loss 0.308, val_loss 0.338\n",
      "GRU Epoch 13, train_loss 0.308, val_loss 0.334\n",
      "GRU Epoch 14, train_loss 0.308, val_loss 0.362\n",
      "GRU Epoch 15, train_loss 0.308, val_loss 0.364\n",
      "GRU Epoch 16, train_loss 0.307, val_loss 0.318\n",
      "GRU Epoch 17, train_loss 0.307, val_loss 0.318\n",
      "BEST ----> \n",
      "GRU Epoch 18, train_loss 0.307, val_loss 0.314\n",
      "GRU Epoch 19, train_loss 0.307, val_loss 0.34\n",
      "GRU Epoch 20, train_loss 0.307, val_loss 0.333\n",
      "GRU Epoch 21, train_loss 0.307, val_loss 0.325\n",
      "GRU Epoch 22, train_loss 0.307, val_loss 0.33\n",
      "GRU Epoch 23, train_loss 0.307, val_loss 0.323\n",
      "GRU Epoch 24, train_loss 0.307, val_loss 0.33\n",
      "GRU Epoch 25, train_loss 0.307, val_loss 0.38\n",
      "GRU Epoch 26, train_loss 0.307, val_loss 0.326\n",
      "GRU Epoch 27, train_loss 0.306, val_loss 0.328\n",
      "GRU Epoch 28, train_loss 0.306, val_loss 0.483\n",
      "GRU Epoch 29, train_loss 0.306, val_loss 0.323\n",
      "GRU Epoch 30, train_loss 0.306, val_loss 0.329\n",
      "GRU Epoch 31, train_loss 0.306, val_loss 0.327\n",
      "GRU Epoch 32, train_loss 0.306, val_loss 0.46\n",
      "GRU Epoch 33, train_loss 0.306, val_loss 0.437\n",
      "GRU Epoch 34, train_loss 0.306, val_loss 0.332\n",
      "GRU Epoch 35, train_loss 0.306, val_loss 0.364\n",
      "GRU Epoch 36, train_loss 0.306, val_loss 0.335\n",
      "GRU Epoch 37, train_loss 0.306, val_loss 0.459\n",
      "GRU Epoch 38, train_loss 0.306, val_loss 0.39\n",
      "GRU Epoch 39, train_loss 0.306, val_loss 0.319\n",
      "GRU Epoch 40, train_loss 0.306, val_loss 0.344\n",
      "GRU Epoch 41, train_loss 0.306, val_loss 0.417\n",
      "GRU Epoch 42, train_loss 0.306, val_loss 0.393\n",
      "GRU Epoch 43, train_loss 0.306, val_loss 0.441\n",
      "GRU Epoch 44, train_loss 0.306, val_loss 0.327\n",
      "GRU Epoch 45, train_loss 0.306, val_loss 0.346\n",
      "GRU Epoch 46, train_loss 0.306, val_loss 0.404\n",
      "GRU Epoch 47, train_loss 0.306, val_loss 0.335\n",
      "GRU Epoch 48, train_loss 0.306, val_loss 0.325\n"
     ]
    }
   ],
   "source": [
    "x_val, y_val, trained_models = cross_validate_models(data, epochs=100, clip_norm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-underwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lstm = average_prediction(x_val[:n], [trained_models[-1]]).squeeze()\n",
    "pred_gru = average_prediction(x_val[:n], [trained_models[-2]]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"lstm error:\", np.linalg.norm(y_val[:n]-pred_lstm))\n",
    "print(\"gru error:\", np.linalg.norm(y_val[:n]-pred_gru))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_val[:n])\n",
    "plt.plot(pred_lstm)\n",
    "plt.plot(pred_gru)\n",
    "plt.legend([\"true\", \"pred_lstm\", \"pred_gru\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-baptist",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecmldeep",
   "language": "python",
   "name": "ecmldeep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
